# -*- coding: utf-8 -*-
"""Phân tích dự báo bằng XGboost.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1hZK1f8Fmg_MWq4FYXglDr9JTr4w1m-76
"""

!pip install xgboost scikit-learn pandas

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

import pandas as pd

# Đọc dữ liệu
features = pd.read_csv("/content/Features data set.csv")
sales = pd.read_csv("/content/sales data-set.csv")

# Chuyển định dạng ngày về cùng kiểu datetime
features["Date"] = pd.to_datetime(features["Date"], dayfirst=True)
sales["Date"] = pd.to_datetime(sales["Date"], dayfirst=True)

# Sort dataframes by date
features = features.sort_values(by='Date')
sales = sales.sort_values(by='Date')

# Convert 'IsHoliday' to integer (0 or 1)
features['IsHoliday'] = features['IsHoliday'].astype(int)
sales['IsHoliday'] = sales['IsHoliday'].astype(int)


# Kiểm tra dữ liệu
print(features.head())
print(sales.head())

merged_df = pd.merge(features, sales, on=['Store', 'Date','IsHoliday'], how='left')
display(merged_df)

merged_df.isna().sum()

merged_df.duplicated().sum()

merged_df = merged_df.drop(columns=['MarkDown1', 'MarkDown2', 'MarkDown3', 'MarkDown4', 'MarkDown5'])

merged_df

merged_df.isna().sum()

merged_df.dropna(subset=['Weekly_Sales'], inplace=True)
display(merged_df)

merged_df.duplicated().sum()

merged_df.isna().sum()

merged_df['Date'] = pd.to_datetime(merged_df['Date'])
merged_df['IsHoliday'] = merged_df['IsHoliday'].astype(int)
merged_df['Dept'] = merged_df['Dept'].astype(int)

for col in ['Temperature','Fuel_Price','CPI','Unemployment','IsHoliday','Dept']:
    plt.figure(figsize=(6,3))
    sns.boxplot(x=merged_df[col])
    plt.title(col)
    plt.show()

def cap_outliers(series):
    Q1 = series.quantile(0.25)
    Q3 = series.quantile(0.75)
    IQR = Q3 - Q1
    lower = Q1 - 1.5 * IQR
    upper = Q3 + 1.5 * IQR
    return np.clip(series, lower, upper)

for col in ['Temperature','Fuel_Price','CPI','Unemployment']:
    merged_df[col] = cap_outliers(merged_df[col])

merged_df['Weekly_Sales'] = merged_df['Weekly_Sales'].clip(lower=0)   # tránh log âm
merged_df['Weekly_Sales_Log'] = np.log1p(merged_df['Weekly_Sales'])

from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
scaled_cols = ['Temperature', 'Fuel_Price', 'CPI', 'Unemployment','IsHoliday','Dept']

merged_df[scaled_cols] = scaler.fit_transform(merged_df[scaled_cols])

from sklearn.model_selection import train_test_split

X = merged_df.drop(['Weekly_Sales'], axis=1)
y = merged_df['Weekly_Sales']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

from statsmodels.stats.outliers_influence import variance_inflation_factor
import pandas as pd

X_vif = X_train.copy()
# Drop the 'Date' column as VIF is for numerical features
X_vif = X_vif.drop('Date', axis=1)
vif_data = pd.DataFrame()
vif_data['Feature'] = X_vif.columns
vif_data['VIF'] = [variance_inflation_factor(X_vif.values, i) for i in range(X_vif.shape[1])]
print(vif_data)

import matplotlib.pyplot as plt
import pandas as pd
# Danh sách các biến độc lập muốn vẽ scatter
features = ['IsHoliday',"Temperature", "Fuel_Price", "CPI", "Unemployment", "Dept"]

# Vẽ biểu đồ scatter cho từng biến
for col in features:
    plt.figure(figsize=(7, 6))
    plt.scatter(merged_df[col], merged_df["Weekly_Sales"])
    plt.xlabel(col)
    plt.ylabel("Weekly_Sales")
    plt.title(f"Scatter Plot: Weekly_Sales vs {col}")
    plt.grid(True)
    plt.show()

# Các cột số liên tục cần kiểm tra ngoại lai
numeric_cols = ["Temperature", "Fuel_Price", "CPI", "Unemployment", "Weekly_Sales"]

# Vẽ boxplot trước xử lý ngoại lai
plt.figure(figsize=(12, 6))
sns.boxplot(data=merged_df[numeric_cols])
plt.title("Boxplot trước xử lý ngoại lai")
plt.show()

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# Hàm giới hạn giá trị ngoại lai theo IQR
def cap_outliers(series):
    Q1 = series.quantile(0.25)
    Q3 = series.quantile(0.75)
    IQR = Q3 - Q1
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR
    return np.where(series < lower_bound, lower_bound,
                    np.where(series > upper_bound, upper_bound, series))

# Áp dụng giới hạn ngoại lai cho các biến số, trừ Weekly_Sales
for col in ["Temperature", "Fuel_Price", "CPI", "Unemployment"]:
    merged_df[col] = cap_outliers(merged_df[col])

# Áp dụng log-transform cho Weekly_Sales để giảm ảnh hưởng ngoại lai
merged_df["Weekly_Sales_Log"] = np.log1p(merged_df["Weekly_Sales"])

merged_df.info()

merged_df[merged_df["Weekly_Sales"] < 0][["Store","Dept","Weekly_Sales"]]

merged_df.to_csv('data_cleaned.csv', index=False)

"""### Chuẩn hóa dữ liệu (Data Standardization)

Tôi sẽ chuẩn hóa các cột số (`Temperature`, `Fuel_Price`, `CPI`, `Unemployment`, `Store`, `Dept`) bằng cách sử dụng `StandardScaler` từ thư viện `sklearn.preprocessing`. Cột `IsHoliday` đã là nhị phân (0 hoặc 1) nên không cần chuẩn hóa theo cách này, và `Weekly_Sales` là biến mục tiêu nên sẽ được giữ nguyên.
"""

from sklearn.preprocessing import StandardScaler

# Tạo một bản sao của merged_df để không làm thay đổi DataFrame gốc
df_scaled = merged_df.copy()

# Xác định các cột số cần chuẩn hóa (loại trừ Weekly_Sales và IsHoliday)
numerical_features_to_scale = ['Store', 'Temperature', 'Fuel_Price', 'CPI', 'Unemployment', 'Dept']

# Khởi tạo StandardScaler
scaler = StandardScaler()

# Áp dụng StandardScaler lên các cột đã chọn
df_scaled[numerical_features_to_scale] = scaler.fit_transform(df_scaled[numerical_features_to_scale])

# Hiển thị 5 hàng đầu tiên của DataFrame đã chuẩn hóa
display(df_scaled.head())

# Select only numerical columns for correlation calculation
numerical_cols = df_scaled.select_dtypes(include=['number'])

# Calculate the correlation matrix
correlation_matrix = numerical_cols.corr()
# Vẽ heatmap thể hiện mức độ tương quan
plt.figure(figsize=(10,6))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=".2f", linewidths=0.5)
plt.title("Ma trận tương quan giữa các biến số", fontsize=14)
plt.show()

"""# phân tích mô tả bộ dữ liệu"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

df = pd.read_csv('/content/data_cleaned (1).csv')

#  Chọn các cột dữ liệu số (numeric columns)
df_numeric = df.select_dtypes(include=['number'])
# đếm các dữ liệu không bị khuyết
data_count = df_numeric.count()
# Tính và in ra trung bình cộng theo hàng (axis=1)
row_means = df_numeric.mean(axis=1)
# Tính và in ra trung bình cộng theo cột (axis=0)
column_means = df_numeric.mean(axis=0)
# Tính median của từng cột
column_medians = df_numeric.median()
# Tính mode của từng cột
column_modes = df_numeric.mode()
# Tính giá trị max của từng cột
column_max = df_numeric.max()
# Tính giá trị min của từng cột
column_min = df_numeric.min()
# Tính Q1, Q2 , Q3 cho từng cột
column_q1 = df_numeric.quantile(0.25)
column_q2 = df_numeric.median()
column_q3 = df_numeric.quantile(0.75)
column_IQR = column_q3 - column_q1
# Tính phương sai của từng cột
column_variances = df_numeric.var()
# Tính độ lệch chuẩn của từng cột
column_std_devs = df_numeric.std()
# Tạo bảng thống kê (tự tạo bảng để giống với bảng đề bài yêu cầu)
def descriptive(data_count,column_min,column_max,column_medians,data_means,column_modes,column_q1,column_q2,column_q3,column_IQR,column_variances,column_std_devs):
        data = {'Count': [i for i in data_count ],
                'min': [i for i in column_min ],
                'max': [i for i in column_max ],
                'median': [i for i in column_medians ],
                'mean': [i for i in data_means ],
                'mode': [i for i in column_modes.values[0]],
                'Q1': [i for i in column_q1 ],
                'Q2': [i for i in column_q2 ],
                'Q3': [i for i in column_q3 ],
                'IQR': [i for i in column_IQR ],
                'Variance': [i for i in column_variances ],
                'stdev': [i for i in column_std_devs ],
                } # dữ liệu đang ở dạng dic
        df1 = pd.DataFrame(data) # convert về dạng pandas
        df1.index=df_numeric.keys() # keys sẽ trả về tên của các cột( features)
        data_complete = df1.transpose() # transpose để chuyển hàng về cột, cột về hàng
# Thêm một cột mới vào đầu DataFrame
        new_column_data = ['count','min','max','median','means','mode','Q1','Q2','Q3','IQR','Variance','stdev']
        column_name = ' '
        data_complete.insert(loc=0, column=column_name, value=new_column_data)
        print(data_complete)
        data_complete.to_csv('Thong_ke_1.txt', sep='\t', index=False)

descriptive(data_count,column_min,column_max,column_medians,column_means,column_modes,column_q1,column_q2,column_q3,column_IQR,column_variances,column_std_devs)
print('---------------------------------------------------------------------------------------------------------------------------------------------')

"""Doanh số (Weekly_Sales):

Trung bình: Khoảng 15,981 USD.

Trung vị (Median): 7,612 USD (thấp hơn nhiều so với trung bình, cho thấy phân phối lệch phải).

Độ lệch chuẩn: Rất lớn (22,711 USD), cho thấy sự biến động mạnh trong doanh số bán hàng.

Khoảng giá trị: Từ mức âm (-4,988 USD) đến rất cao (693,099 USD).
"""

# Chuyển đổi cột Date sang định dạng datetime để vẽ biểu đồ thời gian chính xác
df['Date'] = pd.to_datetime(df['Date'])

# Thiết lập giao diện (style) cho biểu đồ đẹp hơn
sns.set(style="whitegrid")

# ---------------------------------------------------------
# Biểu đồ 1: Phân phối Weekly Sales (Histogram)
# ---------------------------------------------------------
plt.figure(figsize=(12, 6))
sns.histplot(df['Weekly_Sales'], bins=100, kde=True)
plt.title('Phân phối Weekly Sales (Doanh số hàng tuần)')
plt.xlabel('Weekly Sales')
plt.ylabel('Tần suất (Frequency)')
plt.xlim(-5000, 100000) # Giới hạn trục x để tập trung vào phần dữ liệu chính
plt.show()

# ---------------------------------------------------------
# Biểu đồ 2: Tổng doanh số hàng tuần theo thời gian (Line Plot)
# ---------------------------------------------------------
plt.figure(figsize=(12, 6))
# Gom nhóm theo ngày và tính tổng doanh số
weekly_sales_time = df.groupby('Date')['Weekly_Sales'].sum().reset_index()
sns.lineplot(x='Date', y='Weekly_Sales', data=weekly_sales_time)
plt.title('Tổng doanh số hàng tuần theo thời gian')
plt.xlabel('Ngày')
plt.ylabel('Tổng doanh số (Total Weekly Sales)')
plt.show()

# ---------------------------------------------------------
# Biểu đồ 3: Ma trận tương quan (Heatmap)
# ---------------------------------------------------------
plt.figure(figsize=(10, 8))
# Chọn các cột số quan trọng để tính tương quan
corr_cols = ['Weekly_Sales', 'Temperature', 'Fuel_Price', 'CPI', 'Unemployment', 'IsHoliday']
sns.heatmap(df[corr_cols].corr(), annot=True, cmap='coolwarm', fmt=".2f")
plt.title('Ma trận tương quan (Correlation Heatmap)')
plt.show()

# Biểu đồ 4: Doanh số trung bình theo Cửa hàng (Đã thêm màu)
# ---------------------------------------------------------
plt.figure(figsize=(14, 6))
store_sales = df.groupby('Store')['Weekly_Sales'].mean().reset_index().sort_values('Weekly_Sales', ascending=False)

# Thêm tham số palette='viridis' để tạo dải màu đẹp mắt
# Thêm hue='Store' và legend=False để tránh cảnh báo trong các phiên bản Seaborn mới
sns.barplot(
    x='Store',
    y='Weekly_Sales',
    data=store_sales,
    order=store_sales['Store'],
    palette='viridis',
    hue='Store',
    legend=False
)

plt.title('Doanh số trung bình hàng tuần theo Cửa hàng (Store)')
plt.xlabel('Store')
plt.ylabel('Doanh số trung bình (Average Weekly Sales)')
plt.show()

# ---------------------------------------------------------
# Biểu đồ 5: So sánh Ngày lễ vs Ngày thường (Đã thêm màu)
# ---------------------------------------------------------
plt.figure(figsize=(8, 6))

# Thêm tham số palette='Set2' để phân biệt rõ 2 nhóm bằng màu sắc
sns.boxplot(
    x='IsHoliday',
    y='Weekly_Sales',
    data=df,
    showfliers=False,
    palette='Set2',
    hue='IsHoliday',
    legend=False
)

plt.title('Phân phối doanh số: Ngày lễ vs Ngày thường')
plt.xlabel('Is Holiday (0=Không, 1=Có)')
plt.ylabel('Weekly Sales')
plt.show()

"""1. Biểu đồ Phân phối Doanh số (Histogram)
Nhận định: Dữ liệu bị lệch phải (Right-skewed) rất nặng.

Chi tiết: Đa số các giao dịch hàng tuần tại các bộ phận (Dept) đều có giá trị nhỏ (dưới 20.000 USD). Tuy nhiên, đuôi biểu đồ kéo dài về phía bên phải cho thấy sự tồn tại của những tuần "bùng nổ" với doanh số cực lớn (có thể lên tới hàng trăm nghìn USD).

Ý nghĩa kinh doanh: Không nên chỉ nhìn vào doanh số trung bình (mean) để đánh giá hiệu quả, vì con số này bị kéo lên bởi các tuần ngoại lệ. Cần chú ý quản lý kho hàng linh hoạt: mức dự trữ thông thường cho đa số các tuần, nhưng phải có kịch bản ứng phó riêng cho các tuần cao điểm đột biến để tránh "cháy hàng".

2. Biểu đồ Doanh số theo Thời gian (Line Plot)
Nhận định: Tính mùa vụ (Seasonality) là yếu tố chi phối mạnh nhất.

Chi tiết: Có những đỉnh nhọn lặp lại rất rõ ràng vào cuối năm (tháng 11 và 12), tương ứng với Lễ Tạ Ơn và Giáng Sinh. Ngay sau đó là các đợt sụt giảm mạnh vào đầu năm mới.

Ý nghĩa kinh doanh: Chiến lược marketing và nhân sự cần tập trung tối đa nguồn lực vào Q4. Các chương trình khuyến mãi vào tháng 1, tháng 2 nên được thiết kế để kích cầu trong giai đoạn thấp điểm này (ví dụ: xả hàng tồn sau tết, khuyến mãi đầu năm).

3. Biểu đồ Ma trận Tương quan (Heatmap)
Nhận định: Các yếu tố vĩ mô ít ảnh hưởng trực tiếp đến doanh số ngắn hạn.

Chi tiết: Màu sắc nhạt nhòa giữa Weekly_Sales và các biến như Temperature, Fuel_Price, CPI, Unemployment cho thấy hệ số tương quan rất thấp (gần bằng 0).

Ý nghĩa kinh doanh: Người tiêu dùng vẫn đi siêu thị mua sắm bất kể trời nóng hay lạnh, giá xăng tăng hay giảm nhẹ trong tuần đó. Do đó, thay vì lo lắng về các yếu tố vĩ mô khó kiểm soát này, cửa hàng nên tập trung vào các yếu tố nội tại có thể kiểm soát được như: vị trí trưng bày, chất lượng phục vụ, và các chương trình khuyến mãi tại điểm bán.

4. Biểu đồ Doanh số theo Cửa hàng (Bar Chart)
Nhận định: Sự chênh lệch hiệu suất giữa các cửa hàng là rất lớn (quy tắc 80/20).

Chi tiết: Các cửa hàng top đầu (như Store 20, 4, 14) có doanh số gấp nhiều lần so với các cửa hàng nhóm dưới (như Store 33, 5).

Ý nghĩa kinh doanh: Cần tìm hiểu "công thức thành công" của các cửa hàng top đầu (Vị trí của họ ở đâu? Quy mô dân số quanh đó? Cách quản lý?) để áp dụng cho các cửa hàng khác. Đối với các cửa hàng hiệu suất thấp, cần xem xét lại chiến lược: liệu có nên tái cấu trúc, giảm quy mô hay thay đổi danh mục sản phẩm cho phù hợp với địa phương không?

5. Biểu đồ Ngày lễ vs Ngày thường (Boxplot)
Nhận định: Ngày lễ mang lại cơ hội đột phá doanh thu (outliers).

Chi tiết: Mặc dù mức trung vị (đường giữa hộp) của ngày lễ và ngày thường không chênh lệch quá lớn, nhưng biểu đồ ngày lễ xuất hiện rất nhiều điểm ngoại lai (chấm đen) ở phía trên cao.

Ý nghĩa kinh doanh: Ngày lễ là dịp để "cá kiếm". Khách hàng có xu hướng chi tiêu mạnh tay hơn hẳn mức bình thường. Các cửa hàng cần chuẩn bị các gói sản phẩm quà tặng, combo giá trị cao (high-ticket items) vào các dịp này vì tâm lý khách hàng sẵn sàng chi trả lớn hơn.
"""

import pandas as pd
import xgboost as xgb
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error

df = pd.read_csv('/content/data_cleaned (1).csv')

# Chọn biến độc lập và biến phụ thuộc
features = ['Store', 'Dept', 'IsHoliday', 'Temperature', 'Fuel_Price', 'CPI', 'Unemployment']
X = df[features]
y = df['Weekly_Sales']

import xgboost as xgb
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
import numpy as np
import matplotlib.pyplot as plt

results = {}  # lưu kết quả
random_states = range(40, 50)

feature_importance_list = []  # lưu importance để lấy trung bình

for rs in random_states:
    # Tách dữ liệu
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=rs
    )

    # Model XGBoost
    model = xgb.XGBRegressor(
        objective='reg:squarederror',
        n_estimators=100,
        learning_rate=0.1,
        max_depth=5,
        subsample=0.8,
        colsample_bytree=0.8,
        random_state=rs
    )

    # Train
    model.fit(X_train, y_train)

    # Predict
    y_pred = model.predict(X_test)

    # Metrics
    mse = mean_squared_error(y_test, y_pred)
    rmse = np.sqrt(mse)
    mae = mean_absolute_error(y_test, y_pred)
    r2 = r2_score(y_test, y_pred)

    # Lưu chỉ số
    results[f"rs_{rs}"] = {
        "MSE": mse,
        "RMSE": rmse,
        "MAE": mae,
        "R2": r2
    }

    # Lưu importance
    feature_importance_list.append(model.feature_importances_)

# --------------------------------------------------------
# 1) TÍNH TRUNG BÌNH CHỈ SỐ QUA 10 LẦN
# --------------------------------------------------------
avg_mse = np.mean([results[k]["MSE"] for k in results])
avg_rmse = np.mean([results[k]["RMSE"] for k in results])
avg_mae = np.mean([results[k]["MAE"] for k in results])
avg_r2 = np.mean([results[k]["R2"] for k in results])

print("\n=== TRUNG BÌNH CHỈ SỐ QUA 10 RANDOM STATE ===")
print(f"MSE trung bình : {avg_mse:.4f}")
print(f"RMSE trung bình: {avg_rmse:.4f}")
print(f"MAE trung bình : {avg_mae:.4f}")
print(f"R2 trung bình  : {avg_r2:.4f}")

# --------------------------------------------------------
# 2) TÍNH TRUNG BÌNH FEATURE IMPORTANCE
# --------------------------------------------------------
avg_feature_importance = np.mean(feature_importance_list, axis=0)

# Sắp xếp giảm dần
sorted_idx = np.argsort(avg_feature_importance)[::-1]  # reverse để giảm dần

sorted_features = X.columns[sorted_idx]
sorted_importance = avg_feature_importance[sorted_idx]

# ---------------------------------------
# VẼ BIỂU ĐỒ
# ---------------------------------------
plt.figure(figsize=(10, 6))
plt.barh(sorted_features, sorted_importance)
plt.gca().invert_yaxis()  # để feature lớn nhất nằm trên cùng
plt.xlabel("Feature Importance (Average of 10 runs)")
plt.title("XGBoost Feature Importance (Sorted - High → Low)")
plt.tight_layout()
plt.show()

from sklearn.linear_model import LinearRegression

# Lưu kết quả Linear Regression
lr_results = {}
lr_coefs_list = []

for rs in random_states:
    # Tách dữ liệu
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=rs
    )

    # Model Linear Regression
    lr_model = LinearRegression()
    lr_model.fit(X_train, y_train)

    # Predict
    y_pred_lr = lr_model.predict(X_test)

    # Metrics
    mse = mean_squared_error(y_test, y_pred_lr)
    rmse = np.sqrt(mse)
    mae = mean_absolute_error(y_test, y_pred_lr)
    r2 = r2_score(y_test, y_pred_lr)

    # Lưu chỉ số
    lr_results[f"rs_{rs}"] = {
        "MSE": mse,
        "RMSE": rmse,
        "MAE": mae,
        "R2": r2
    }

    # Lưu hệ số để có thể tính trung bình
    lr_coefs_list.append(lr_model.coef_)

# --------------------------------------------------------
# 1) TÍNH TRUNG BÌNH CHỈ SỐ LINEAR REGRESSION
# --------------------------------------------------------
avg_mse_lr = np.mean([lr_results[k]["MSE"] for k in lr_results])
avg_rmse_lr = np.mean([lr_results[k]["RMSE"] for k in lr_results])
avg_mae_lr = np.mean([lr_results[k]["MAE"] for k in lr_results])
avg_r2_lr = np.mean([lr_results[k]["R2"] for k in lr_results])

print("\n=== LINEAR REGRESSION (TRUNG BÌNH 10 RANDOM STATE) ===")
print(f"MSE trung bình : {avg_mse_lr:.4f}")
print(f"RMSE trung bình: {avg_rmse_lr:.4f}")
print(f"MAE trung bình : {avg_mae_lr:.4f}")
print(f"R2 trung bình  : {avg_r2_lr:.4f}")

# --------------------------------------------------------
# 2) TÍNH TRUNG BÌNH HỆ SỐ COEF
# --------------------------------------------------------
avg_lr_coef = np.mean(lr_coefs_list, axis=0)

# In ra bảng hệ số trung bình
for f, coef in zip(X.columns, avg_lr_coef):
    print(f"{f:15}: {coef:.4f}")

# --------------------------------------------------------
# 3) So sánh XGBoost và Linear Regression
# --------------------------------------------------------
print("\n=== SO SÁNH TRUNG BÌNH METRICS ===")
print(f"{'Metric':<10} | {'LinearReg':>12} | {'XGBoost':>12}")
print("-"*40)
print(f"{'MSE':<10} | {avg_mse_lr:12.4f} | {avg_mse:12.4f}")
print(f"{'RMSE':<10} | {avg_rmse_lr:12.4f} | {avg_rmse:12.4f}")
print(f"{'MAE':<10} | {avg_mae_lr:12.4f} | {avg_mae:12.4f}")
print(f"{'R2':<10} | {avg_r2_lr:12.4f} | {avg_r2:12.4f}")

